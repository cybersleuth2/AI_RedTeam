ðŸš€ https://github.com/ottosulin/awesome-ai-security  
ðŸš€ https://github.com/Hannibal046/Awesome-LLM  
ðŸš€ https://github.com/greshake/llm-security    
ðŸš€ https://github.com/ScottLogic/prompt-injection  
ðŸš€ https://github.com/ReversecLabs/damn-vulnerable-llm-agent  
ðŸš€ https://github.com/mik0w/pallms  

https://vickieli.medium.com/hacking-llms-with-prompt-injections-6a5ebffb182b  
https://aivillage.org/large%20language%20models/threat-modeling-llm/  
https://portswigger.net/web-security/llm-attacks  
https://gandalf.lakera.ai/  
https://www.ibm.com/think/topics/prompt-injection  
https://learnprompting.org/docs/prompt_hacking/injection  
https://llmsecurity.net/  
https://genai.owasp.org/  
https://www.promptingguide.ai/risks/adversarial  
https://www.promptingguide.ai/research/rag  
https://www.cobalt.io/blog/prompt-injection-attacks  
https://www.bugcrowd.com/blog/ai-vulnerability-deep-dive-prompt-injection/  
https://www.unite.ai/prompt-hacking-and-misuse-of-llm/?trk=article-ssr-frontend-pulse_little-text-block    
https://simonwillison.net/2023/May/2/prompt-injection-explained/  

